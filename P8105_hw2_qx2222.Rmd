---
title: "P8105_hw2_qx2222"
author: "Qiaoyi Xu"
date: "2022-10-02"
output: github_document
---

```{r load_libraries}
library(tidyverse)
library(readxl)
```



### Problem 1 (answer posted)

Below we import and clean data from `NYC_Transit_Subway_Entrance_And_Exit_Data.csv`. The process begins with data import, updates variable names, and selects the columns that will be used in later parts fo this problem. We update `entry` from `yes` / `no` to a logical variable. As part of data import, we specify that `Route` columns 8-11 should be character for consistency with 1-7.

```{r}
trans_ent = 
  read_csv(
    "data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
    col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) %>% 
  janitor::clean_names() %>% 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) %>% 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

As it stands, these data are not "tidy": route number should be a variable, as should route. That is, to obtain a tidy dataset we would need to convert `route` variables from wide to long format. This will be useful when focusing on specific routes, but may not be necessary when considering questions that focus on station-level variables. 

The following code chunk selects station name and line, and then uses `distinct()` to obtain all unique combinations. As a result, the number of rows in this dataset is the number of unique stations.

```{r}
trans_ent %>% 
  select(station_name, line) %>% 
  distinct
```

The next code chunk is similar, but filters according to ADA compliance as an initial step. This produces a dataframe in which the number of rows is the number of ADA compliant stations. 

```{r}
trans_ent %>% 
  filter(ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```

To compute the proportion of station entrances / exits without vending allow entrance, we first exclude station entrances that do not allow vending. Then, we focus on the `entry` variable -- this logical, so taking the mean will produce the desired proportion (recall that R will coerce logical to numeric in cases like this).

```{r}
trans_ent %>% 
  filter(vending == "NO") %>% 
  pull(entry) %>% 
  mean
```

Lastly, we write a code chunk to identify stations that serve the A train, and to assess how many of these are ADA compliant. As a first step, we tidy the data as alluded to previously; that is, we convert `route` from wide to long format. After this step, we can use tools from previous parts of the question (filtering to focus on the A train, and on ADA compliance; selecting and using `distinct` to obtain dataframes with the required stations in rows).

```{r}
trans_ent %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A") %>% 
  select(station_name, line) %>% 
  distinct

trans_ent %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A", ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct

```



### Problem 2
we import and clean data from `Trash Wheel Collection Data.xlsx`： 

```{r, warning=FALSE}
mr_trash_wheel = 
  read_excel( 
    "data/Trash Wheel Collection Data.xlsx",
    sheet = "Mr. Trash Wheel", range = "A2:N535") %>% # omit non-data entries
  janitor::clean_names() %>%
  mutate (dumpster = as.numeric(dumpster)) %>%
  drop_na(dumpster)%>% #omit rows that do not include dumpster-specific data
  mutate (sports_balls = round(sports_balls), sports_balls = as.integer(sports_balls), from_sheet = 'mr') #round and convert to integer variable

mr_trash_wheel
    
```


Use a similar process to import, clean, and organize the data for Professor Trash Wheel：
```{r}
prof_trash_wheel = 
  read_excel( 
    "data/Trash Wheel Collection Data.xlsx",
    sheet = "Professor Trash Wheel", range = "A2:M97") %>% # omit non-data entries
  janitor::clean_names() %>%
  drop_na(dumpster)%>% #omit rows that do not include dumpster-specific data
  mutate(year = as.character(year), sports_balls = NA , sports_balls = as.integer(sports_balls), from_sheet = "Professor")

prof_trash_wheel
```

```{r}
trash_wheel_tidy = bind_rows(mr_trash_wheel, prof_trash_wheel) #combine two datasets

trash_wheel_tidy
```


```{r}
#filter combined dataset to answer questions in descriptive part
total_weight_prof = filter (trash_wheel_tidy, from_sheet == 'Professor')
mr_2020 = filter(trash_wheel_tidy, from_sheet == 'mr', year == 2020)
```

#descriptive paragraph:


In the combined dataset(trash_wheel_tidy), there are 524 observations and 15 variables. Key variables include `r names(trash_wheel_tidy)`.
The total weight of trash collected by Professor Trash Wheel is `r sum(total_weight_prof$weight_tons)` tons.
The total number of sports balls collected by Mr. Trash Wheel in 2020 is `r sum(mr_2020$sports_balls)` balls.


### Problem 3

First, clean the data in pols-month.csv.

```{r}
pols_month = 
  read_csv("data/fivethirtyeight_datasets/pols-month.csv") %>%
  janitor::clean_names() %>% 
  separate(mon, into = c("year","month","day")) %>% #break up the mon variable
  mutate(year = as.integer(year), 
         day = as.integer(day), 
         month = month.name[as.numeric(month)], #replace month number with month name
         president = ifelse(prez_dem == 1, "demo", "repub")) %>% #create a president variable
  select(-prez_dem, -prez_gop, -day) #remove prez_dem, prez_gop, and day variables

pols_month
  
```

Second, clean the data in snp.csv using a similar process to the above. 
```{r}
snp = 
  read_csv("data/fivethirtyeight_datasets/snp.csv") %>%
  janitor::clean_names() %>% 
  mutate(date = lubridate::parse_date_time2(date,orders ="mdy", cutoff_2000 = 23),
         date = as.Date(date, format = "%m/%d/%y")) %>%
  separate(date, into = c("year","month","day")) %>% #break up the date variable
  select(-day) %>%
  arrange(year, month) %>% #arrange according to year and month
  mutate(year = as.integer(year), 
         month = month.name[as.numeric(month)]) #replace month number with month name

snp

```

Third, tidy the unemployment data so that it can be merged with the previous datasets. This process will involve switching from “wide” to “long” format;
```{r}
unemploy = 
  read_csv("data/fivethirtyeight_datasets/unemployment.csv") %>%
  janitor::clean_names() %>% 
  rename(January = jan, Feburary = feb, March = mar, April = apr, May = may, June = jun, July = jul,
         August = aug, September = sep, October = oct, November = nov, December = dec) %>%
  pivot_longer(January:December, names_to = "month", values_to = "unemployment") %>%
  mutate(year = as.integer(year))
  
unemploy
```

Join the datasets by merging snp into pols, and merging unemployment into the result.

```{r}
merged = right_join(snp, pols_month, by = c("year", "month"))
merged_into_result = right_join(unemploy, merged, by = c("year", "month")) %>% arrange(year, month)

merged_into_result
  
```


#descriptive paragraph:


In the pols_month dataset, there are `r nrow(pols_month)` observations and `r ncol(pols_month)` variables. Variables include `r names(pols_month)`. This dataset describe the number of national politicians who are democratic or republican between Jan 1947 and June 2015.

In the snp dataset, there are `r nrow(snp)` observations and `r ncol(snp)` variables. Variables include `r names(snp)`. This dataset describe Standard & Poor’s stock market index (S&P) between Jan 1969 and July 2015.

In the unemploy dataset, there are `r nrow(unemploy)` observations and `r ncol(unemploy)` variables. Variables include `r names(unemploy)`. This dataset describe unemployment rate between Jan 1948 and June 2015(exclude the missing data).

In the final emerged dataset (merged_into_result), three datasets, pols_month, snp, and unemploy, are merged together by year and month. There are `r nrow(merged_into_result)` observations and `r ncol(merged_into_result)` variables. Variables include `r names(merged_into_result)`. It contains data between April 1947 and May 2015. Because the year range is different in those three datasets, some missing data existed. 











