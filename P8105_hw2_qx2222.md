P8105_hw2_qx2222
================
Qiaoyi Xu
2022-10-02

``` r
library(tidyverse)
```

    ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
    ## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
    ## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
    ## ✔ tidyr   1.2.0      ✔ stringr 1.4.1 
    ## ✔ readr   2.1.2      ✔ forcats 0.5.2 
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

``` r
library(readxl)
```

### Problem 1 (answer posted)

Below we import and clean data from
`NYC_Transit_Subway_Entrance_And_Exit_Data.csv`. The process begins with
data import, updates variable names, and selects the columns that will
be used in later parts fo this problem. We update `entry` from `yes` /
`no` to a logical variable. As part of data import, we specify that
`Route` columns 8-11 should be character for consistency with 1-7.

``` r
trans_ent = 
  read_csv(
    "data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
    col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) %>% 
  janitor::clean_names() %>% 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) %>% 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

As it stands, these data are not “tidy”: route number should be a
variable, as should route. That is, to obtain a tidy dataset we would
need to convert `route` variables from wide to long format. This will be
useful when focusing on specific routes, but may not be necessary when
considering questions that focus on station-level variables.

The following code chunk selects station name and line, and then uses
`distinct()` to obtain all unique combinations. As a result, the number
of rows in this dataset is the number of unique stations.

``` r
trans_ent %>% 
  select(station_name, line) %>% 
  distinct
```

    ## # A tibble: 465 × 2
    ##    station_name             line    
    ##    <chr>                    <chr>   
    ##  1 25th St                  4 Avenue
    ##  2 36th St                  4 Avenue
    ##  3 45th St                  4 Avenue
    ##  4 53rd St                  4 Avenue
    ##  5 59th St                  4 Avenue
    ##  6 77th St                  4 Avenue
    ##  7 86th St                  4 Avenue
    ##  8 95th St                  4 Avenue
    ##  9 9th St                   4 Avenue
    ## 10 Atlantic Av-Barclays Ctr 4 Avenue
    ## # … with 455 more rows

The next code chunk is similar, but filters according to ADA compliance
as an initial step. This produces a dataframe in which the number of
rows is the number of ADA compliant stations.

``` r
trans_ent %>% 
  filter(ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```

    ## # A tibble: 84 × 2
    ##    station_name                   line           
    ##    <chr>                          <chr>          
    ##  1 Atlantic Av-Barclays Ctr       4 Avenue       
    ##  2 DeKalb Av                      4 Avenue       
    ##  3 Pacific St                     4 Avenue       
    ##  4 Grand Central                  42nd St Shuttle
    ##  5 34th St                        6 Avenue       
    ##  6 47-50th Sts Rockefeller Center 6 Avenue       
    ##  7 Church Av                      6 Avenue       
    ##  8 21st St                        63rd Street    
    ##  9 Lexington Av                   63rd Street    
    ## 10 Roosevelt Island               63rd Street    
    ## # … with 74 more rows

To compute the proportion of station entrances / exits without vending
allow entrance, we first exclude station entrances that do not allow
vending. Then, we focus on the `entry` variable – this logical, so
taking the mean will produce the desired proportion (recall that R will
coerce logical to numeric in cases like this).

``` r
trans_ent %>% 
  filter(vending == "NO") %>% 
  pull(entry) %>% 
  mean
```

    ## [1] 0.3770492

Lastly, we write a code chunk to identify stations that serve the A
train, and to assess how many of these are ADA compliant. As a first
step, we tidy the data as alluded to previously; that is, we convert
`route` from wide to long format. After this step, we can use tools from
previous parts of the question (filtering to focus on the A train, and
on ADA compliance; selecting and using `distinct` to obtain dataframes
with the required stations in rows).

``` r
trans_ent %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A") %>% 
  select(station_name, line) %>% 
  distinct
```

    ## # A tibble: 60 × 2
    ##    station_name                  line           
    ##    <chr>                         <chr>          
    ##  1 Times Square                  42nd St Shuttle
    ##  2 125th St                      8 Avenue       
    ##  3 145th St                      8 Avenue       
    ##  4 14th St                       8 Avenue       
    ##  5 168th St - Washington Heights 8 Avenue       
    ##  6 175th St                      8 Avenue       
    ##  7 181st St                      8 Avenue       
    ##  8 190th St                      8 Avenue       
    ##  9 34th St                       8 Avenue       
    ## 10 42nd St                       8 Avenue       
    ## # … with 50 more rows

``` r
trans_ent %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A", ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```

    ## # A tibble: 17 × 2
    ##    station_name                  line            
    ##    <chr>                         <chr>           
    ##  1 14th St                       8 Avenue        
    ##  2 168th St - Washington Heights 8 Avenue        
    ##  3 175th St                      8 Avenue        
    ##  4 34th St                       8 Avenue        
    ##  5 42nd St                       8 Avenue        
    ##  6 59th St                       8 Avenue        
    ##  7 Inwood - 207th St             8 Avenue        
    ##  8 West 4th St                   8 Avenue        
    ##  9 World Trade Center            8 Avenue        
    ## 10 Times Square-42nd St          Broadway        
    ## 11 59th St-Columbus Circle       Broadway-7th Ave
    ## 12 Times Square                  Broadway-7th Ave
    ## 13 8th Av                        Canarsie        
    ## 14 Franklin Av                   Franklin        
    ## 15 Euclid Av                     Fulton          
    ## 16 Franklin Av                   Fulton          
    ## 17 Howard Beach                  Rockaway

### Problem 2

we import and clean data from `Trash Wheel Collection Data.xlsx`：

``` r
mr_trash_wheel = 
  read_excel( 
    "data/Trash Wheel Collection Data.xlsx",
    sheet = "Mr. Trash Wheel", range = "A2:N535") %>% # omit non-data entries
  janitor::clean_names() %>%
  mutate (dumpster = as.numeric(dumpster)) %>%
  drop_na(dumpster)%>% #omit rows that do not include dumpster-specific data
  mutate (sports_balls = round(sports_balls), sports_balls = as.integer(sports_balls), from_sheet = 'mr') #round and convert to integer variable

mr_trash_wheel
```

    ## # A tibble: 533 × 15
    ##    dumpster month year  date                weight_tons volume…¹ plast…² polys…³
    ##       <dbl> <chr> <chr> <dttm>                    <dbl>    <dbl>   <dbl>   <dbl>
    ##  1        1 May   2014  2014-05-16 00:00:00        4.31       18    1450    1820
    ##  2        2 May   2014  2014-05-16 00:00:00        2.74       13    1120    1030
    ##  3        3 May   2014  2014-05-16 00:00:00        3.45       15    2450    3100
    ##  4        4 May   2014  2014-05-17 00:00:00        3.1        15    2380    2730
    ##  5        5 May   2014  2014-05-17 00:00:00        4.06       18     980     870
    ##  6        6 May   2014  2014-05-20 00:00:00        2.71       13    1430    2140
    ##  7        7 May   2014  2014-05-21 00:00:00        1.91        8     910    1090
    ##  8        8 May   2014  2014-05-28 00:00:00        3.7        16    3580    4310
    ##  9        9 June  2014  2014-06-05 00:00:00        2.52       14    2400    2790
    ## 10       10 June  2014  2014-06-11 00:00:00        3.76       18    1340    1730
    ## # … with 523 more rows, 7 more variables: cigarette_butts <dbl>,
    ## #   glass_bottles <dbl>, grocery_bags <dbl>, chip_bags <dbl>,
    ## #   sports_balls <int>, homes_powered <dbl>, from_sheet <chr>, and abbreviated
    ## #   variable names ¹​volume_cubic_yards, ²​plastic_bottles, ³​polystyrene

Use a similar process to import, clean, and organize the data for
Professor Trash Wheel：

``` r
prof_trash_wheel = 
  read_excel( 
    "data/Trash Wheel Collection Data.xlsx",
    sheet = "Professor Trash Wheel", range = "A2:M97") %>% # omit non-data entries
  janitor::clean_names() %>%
  drop_na(dumpster)%>% #omit rows that do not include dumpster-specific data
  mutate(year = as.character(year), sports_balls = NA , sports_balls = as.integer(sports_balls), from_sheet = "Professor")

prof_trash_wheel
```

    ## # A tibble: 94 × 15
    ##    dumpster month    year  date                weight_…¹ volum…² plast…³ polys…⁴
    ##       <dbl> <chr>    <chr> <dttm>                  <dbl>   <dbl>   <dbl>   <dbl>
    ##  1        1 January  2017  2017-01-02 00:00:00      1.79      15    1950    6080
    ##  2        2 January  2017  2017-01-30 00:00:00      1.58      15    9540   11230
    ##  3        3 February 2017  2017-02-26 00:00:00      2.32      18    8350    9210
    ##  4        4 February 2017  2017-02-26 00:00:00      3.72      15    8590    1030
    ##  5        5 February 2017  2017-02-28 00:00:00      1.45      15    7830    9950
    ##  6        6 March    2017  2017-03-30 00:00:00      1.71      15    8210   10340
    ##  7        7 April    2017  2017-04-01 00:00:00      1.82      15    9830   11020
    ##  8        8 April    2017  2017-04-20 00:00:00      2.37      15    9240    8760
    ##  9        9 May      2017  2017-05-10 00:00:00      2.64      15    9540    8810
    ## 10       10 May      2017  2017-05-26 00:00:00      2.78      15    8230    7800
    ## # … with 84 more rows, 7 more variables: cigarette_butts <dbl>,
    ## #   glass_bottles <dbl>, grocery_bags <dbl>, chip_bags <dbl>,
    ## #   homes_powered <dbl>, sports_balls <int>, from_sheet <chr>, and abbreviated
    ## #   variable names ¹​weight_tons, ²​volume_cubic_yards, ³​plastic_bottles,
    ## #   ⁴​polystyrene

``` r
trash_wheel_tidy = bind_rows(mr_trash_wheel, prof_trash_wheel) #combine two datasets

trash_wheel_tidy
```

    ## # A tibble: 627 × 15
    ##    dumpster month year  date                weight_tons volume…¹ plast…² polys…³
    ##       <dbl> <chr> <chr> <dttm>                    <dbl>    <dbl>   <dbl>   <dbl>
    ##  1        1 May   2014  2014-05-16 00:00:00        4.31       18    1450    1820
    ##  2        2 May   2014  2014-05-16 00:00:00        2.74       13    1120    1030
    ##  3        3 May   2014  2014-05-16 00:00:00        3.45       15    2450    3100
    ##  4        4 May   2014  2014-05-17 00:00:00        3.1        15    2380    2730
    ##  5        5 May   2014  2014-05-17 00:00:00        4.06       18     980     870
    ##  6        6 May   2014  2014-05-20 00:00:00        2.71       13    1430    2140
    ##  7        7 May   2014  2014-05-21 00:00:00        1.91        8     910    1090
    ##  8        8 May   2014  2014-05-28 00:00:00        3.7        16    3580    4310
    ##  9        9 June  2014  2014-06-05 00:00:00        2.52       14    2400    2790
    ## 10       10 June  2014  2014-06-11 00:00:00        3.76       18    1340    1730
    ## # … with 617 more rows, 7 more variables: cigarette_butts <dbl>,
    ## #   glass_bottles <dbl>, grocery_bags <dbl>, chip_bags <dbl>,
    ## #   sports_balls <int>, homes_powered <dbl>, from_sheet <chr>, and abbreviated
    ## #   variable names ¹​volume_cubic_yards, ²​plastic_bottles, ³​polystyrene

``` r
#filter combined dataset to answer questions in descriptive part
total_weight_prof = filter (trash_wheel_tidy, from_sheet == 'Professor')
mr_2020 = filter(trash_wheel_tidy, from_sheet == 'mr', year == 2020)
```

\#descriptive paragraph:

In the combined dataset(trash_wheel_tidy), there are 524 observations
and 15 variables. Key variables include dumpster, month, year, date,
weight_tons, volume_cubic_yards, plastic_bottles, polystyrene,
cigarette_butts, glass_bottles, grocery_bags, chip_bags, sports_balls,
homes_powered, from_sheet. The total weight of trash collected by
Professor Trash Wheel is 190.12 tons. The total number of sports balls
collected by Mr. Trash Wheel in 2020 is 856 balls.

### Problem 3

First, clean the data in pols-month.csv.

``` r
pols_month = 
  read_csv("data/fivethirtyeight_datasets/pols-month.csv") %>%
  janitor::clean_names() %>% 
  separate(mon, into = c("year","month","day")) %>% #break up the mon variable
  mutate(year = as.integer(year), 
         day = as.integer(day), 
         month = month.name[as.numeric(month)], #replace month number with month name
         president = ifelse(prez_dem == 1, "demo", "repub")) %>% #create a president variable
  select(-prez_dem, -prez_gop, -day) #remove prez_dem, prez_gop, and day variables
```

    ## Rows: 822 Columns: 9
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl  (8): prez_gop, gov_gop, sen_gop, rep_gop, prez_dem, gov_dem, sen_dem, r...
    ## date (1): mon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
pols_month
```

    ## # A tibble: 822 × 9
    ##     year month     gov_gop sen_gop rep_gop gov_dem sen_dem rep_dem president
    ##    <int> <chr>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>    
    ##  1  1947 January        23      51     253      23      45     198 demo     
    ##  2  1947 February       23      51     253      23      45     198 demo     
    ##  3  1947 March          23      51     253      23      45     198 demo     
    ##  4  1947 April          23      51     253      23      45     198 demo     
    ##  5  1947 May            23      51     253      23      45     198 demo     
    ##  6  1947 June           23      51     253      23      45     198 demo     
    ##  7  1947 July           23      51     253      23      45     198 demo     
    ##  8  1947 August         23      51     253      23      45     198 demo     
    ##  9  1947 September      23      51     253      23      45     198 demo     
    ## 10  1947 October        23      51     253      23      45     198 demo     
    ## # … with 812 more rows

Second, clean the data in snp.csv using a similar process to the above.

``` r
snp = 
  read_csv("data/fivethirtyeight_datasets/snp.csv") %>%
  janitor::clean_names() %>% 
  mutate(date = lubridate::parse_date_time2(date,orders ="mdy", cutoff_2000 = 23),
         date = as.Date(date, format = "%m/%d/%y")) %>%
  separate(date, into = c("year","month","day")) %>% #break up the date variable
  select(-day) %>%
  arrange(year, month) %>% #arrange according to year and month
  mutate(year = as.integer(year), 
         month = month.name[as.numeric(month)]) #replace month number with month name
```

    ## Rows: 787 Columns: 2
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (1): date
    ## dbl (1): close
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
snp
```

    ## # A tibble: 787 × 3
    ##     year month     close
    ##    <int> <chr>     <dbl>
    ##  1  1950 January    17.0
    ##  2  1950 February   17.2
    ##  3  1950 March      17.3
    ##  4  1950 April      18.0
    ##  5  1950 May        18.8
    ##  6  1950 June       17.7
    ##  7  1950 July       17.8
    ##  8  1950 August     18.4
    ##  9  1950 September  19.5
    ## 10  1950 October    19.5
    ## # … with 777 more rows

Third, tidy the unemployment data so that it can be merged with the
previous datasets. This process will involve switching from “wide” to
“long” format;

``` r
unemploy = 
  read_csv("data/fivethirtyeight_datasets/unemployment.csv") %>%
  janitor::clean_names() %>% 
  rename(January = jan, Feburary = feb, March = mar, April = apr, May = may, June = jun, July = jul,
         August = aug, September = sep, October = oct, November = nov, December = dec) %>%
  pivot_longer(January:December, names_to = "month", values_to = "unemployment") %>%
  mutate(year = as.integer(year))
```

    ## Rows: 68 Columns: 13
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (13): Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
unemploy
```

    ## # A tibble: 816 × 3
    ##     year month     unemployment
    ##    <int> <chr>            <dbl>
    ##  1  1948 January            3.4
    ##  2  1948 Feburary           3.8
    ##  3  1948 March              4  
    ##  4  1948 April              3.9
    ##  5  1948 May                3.5
    ##  6  1948 June               3.6
    ##  7  1948 July               3.6
    ##  8  1948 August             3.9
    ##  9  1948 September          3.8
    ## 10  1948 October            3.7
    ## # … with 806 more rows

Join the datasets by merging snp into pols, and merging unemployment
into the result.

``` r
merged = right_join(snp, pols_month, by = c("year", "month"))
merged_into_result = right_join(unemploy, merged, by = c("year", "month")) %>% arrange(year, month)

merged_into_result
```

    ## # A tibble: 822 × 11
    ##     year month    unempl…¹ close gov_gop sen_gop rep_gop gov_dem sen_dem rep_dem
    ##    <int> <chr>       <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>
    ##  1  1947 April          NA    NA      23      51     253      23      45     198
    ##  2  1947 August         NA    NA      23      51     253      23      45     198
    ##  3  1947 December       NA    NA      24      51     253      23      45     198
    ##  4  1947 February       NA    NA      23      51     253      23      45     198
    ##  5  1947 January        NA    NA      23      51     253      23      45     198
    ##  6  1947 July           NA    NA      23      51     253      23      45     198
    ##  7  1947 June           NA    NA      23      51     253      23      45     198
    ##  8  1947 March          NA    NA      23      51     253      23      45     198
    ##  9  1947 May            NA    NA      23      51     253      23      45     198
    ## 10  1947 November       NA    NA      24      51     253      23      45     198
    ## # … with 812 more rows, 1 more variable: president <chr>, and abbreviated
    ## #   variable name ¹​unemployment

\#descriptive paragraph:

In the pols_month dataset, there are 822 observations and 9 variables.
Variables include year, month, gov_gop, sen_gop, rep_gop, gov_dem,
sen_dem, rep_dem, president. This dataset describe the number of
national politicians who are democratic or republican between Jan 1947
and June 2015.

In the snp dataset, there are 787 observations and 3 variables.
Variables include year, month, close. This dataset describe Standard &
Poor’s stock market index (S&P) between Jan 1969 and July 2015.

In the unemploy dataset, there are 816 observations and 3 variables.
Variables include year, month, unemployment. This dataset describe
unemployment rate between Jan 1948 and June 2015(exclude the missing
data).

In the final emerged dataset (merged_into_result), three datasets,
pols_month, snp, and unemploy, are merged together by year and month.
There are 822 observations and 11 variables. Variables include year,
month, unemployment, close, gov_gop, sen_gop, rep_gop, gov_dem, sen_dem,
rep_dem, president. It contains data between April 1947 and May 2015.
Because the year range is different in those three datasets, some
missing data existed.
